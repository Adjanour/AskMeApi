import json
import os

import pinecone
import numpy as np
from ollama import AsyncClient
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer
import spacy
from functools import lru_cache
from typing import List, Dict, Any

from app.db_interface import DBInterface
from app.utils import sanitize_id

# Initialize spaCy English model for NLP
nlp = spacy.load("en_core_web_sm")

# Initialize Sentence Transformer model for embeddings
embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')


# Initialize Pinecone instance with the API key
pc = Pinecone(api_key="pcsk_5k4s6Z_4mMX815ACLEuHfTAKmiDk775uXiUd6NCvNrodqnfbcC3CRQtMTgcqaWGrmpnWTi")


class EmbeddingsHandler:
    def __init__(self, db: DBInterface, index_name: str, embedding_dim: int = 384):
        """
        Initializes the EmbeddingsHandler with the given DB interface and Pinecone index.

        Args:
            db: Database interface to store and retrieve FAQ data.
            index_name: The name of the Pinecone index to use.
            embedding_dim: The dimension of the embeddings generated by the SentenceTransformer.
        """
        self.db = db
        self.embedding_dim = embedding_dim
        self.index_name = index_name
        self.pinecone_index = pc.Index(index_name)  # Initialize Pinecone index using the `pc` instance

    @staticmethod
    def preprocess_text(text: str) -> str:
        """
        Preprocess the input text: lemmatize and remove stopwords.

        Args:
            text: The input text string to preprocess.

        Returns:
            A preprocessed text string with stopwords removed and words lemmatized.
        """
        doc = nlp(text.lower())
        tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]
        return " ".join(tokens)

    def embed_text(self, text: str) -> np.ndarray:
        """
        Generate an embedding for the input text.

        Args:
            text: The input text to embed.

        Returns:
            A numpy array representing the embedding of the input text.
        """
        return embedding_model.encode([text])[0]

    def store_faq_embeddings(self, faqs: List[Dict[str, str]], tenant_id: int):
        """
        Embed and store FAQs in Pinecone index with tenant association.

        Args:
            faqs: A list of FAQ dictionaries containing question and answer.
            tenant_id: The ID of the tenant associated with these FAQs.
        """
        processed_faqs = [
            {
                "question": faq["question"],
                "answer": faq["answer"],
                "processed_question": self.preprocess_text(faq["question"]),
            }
            for faq in faqs
        ]

        # Prepare Pinecone data and store it
        faq_vectors = []
        for faq in processed_faqs:
            question_embedding = self.embed_text(faq["processed_question"])

            # Create the Pinecone vectors to insert into the index
            vector_id = sanitize_id(faq["question"])  # Sanitize vector ID
            faq_vectors.append({
                "id": vector_id,
                "metadata": {"question": faq["question"], "answer": faq["answer"], "tenant_id": tenant_id},
                "values": question_embedding.tolist()
            })

        # Insert into Pinecone
        self.pinecone_index.upsert(vectors=faq_vectors,namespace=tenant_id)

    @lru_cache(maxsize=128)
    async def load_faq_embeddings(self, tenant_id: int):
        """
        Load embeddings for a given tenant ID from the Pinecone index.

        Args:
            tenant_id: The ID of the tenant whose FAQs are being loaded.

        Returns:
            A list of FAQ metadata and their corresponding embeddings.
        """
        # Query Pinecone to get all the vectors associated with the tenant
        query_response = self.pinecone_index.query(
            filter={"tenant_id": tenant_id},
            top_k=100  # Limit to top 100 results (adjust as needed)
        )

        faqs = [{"question": item["metadata"]["question"], "answer": item["metadata"]["answer"]} for item in
                query_response["matches"]]
        embeddings = np.array([item["values"] for item in query_response["matches"]])

        return faqs, embeddings

    async def find_similar_faqs(self, query: str, tenant_id: int, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        Find top similar FAQs for the given query using Pinecone.

        Args:
            query: The user query to compare against stored FAQs.
            tenant_id: The tenant ID used as the namespace.
            top_k: The number of top similar FAQs to retrieve.

        Returns:
            A list of dictionaries containing the most similar FAQs and their scores.
        """
        # Preprocess and embed the user query
        processed_query = self.preprocess_text(query)
        query_embedding = self.embed_text(processed_query).tolist()

        # Query Pinecone using the tenant_id as the namespace
        query_response = self.pinecone_index.query(
            namespace=str(tenant_id),  # Use tenant_id as the namespace
            vector=query_embedding,  # Pass the query embedding
            top_k=top_k,  # Number of results to retrieve
            include_metadata=True  # Include metadata in the response
        )

        # Extract relevant results
        results = [
            {
                "question": item["metadata"]["question"],
                "answer": item["metadata"]["answer"],
                "score": item["score"]  # Similarity score
            }
            for item in query_response["matches"]
        ]

        return results

class LLMHandler:
    @staticmethod
    def generate_llama_prompt(
            similar_faqs: List[Dict[str, Any]],
            user_query: str,
            conversation_history: List[Dict[str, str]] = []
    ) -> str:
        """
        Generate a focused prompt for the LLM using FAQs and conversation history for accurate, relevant responses.

        Args:
            similar_faqs: A list of similar FAQs based on the user's query.
            user_query: The user's current question.
            conversation_history: A list of previous conversation messages.

        Returns:
            A formatted prompt string to send to the LLM for generating a response.
        """
        # Concatenate Q&A pairs from similar FAQs to form the context
        faq_context = "\n\n".join([f"Q: {faq['question']}\nA: {faq['answer']}" for faq in similar_faqs])

        # Format the conversation history context
        conversation_context = "\n".join(
            [f"{message['role'].capitalize()}: {message['content']}" for message in conversation_history])

        # Structured and targeted prompt
        prompt = (
            f"You are a customer support chatbot, designed to give precise, relevant answers based on the provided FAQ. "
            f"Use the FAQ entries and conversation history below to craft a direct, helpful response to the user's question. "
            f"If the exact answer isn't in the FAQ, infer the best possible answer or advise on next steps.\n\n"

            f"FAQs:\n{faq_context}\n\n"
            f"Conversation so far:\n{conversation_context}\n\n"

            f"User question: {user_query}\n\n"

            f"Please provide a clear answer based on the information above. "
            f"Use a polite, concise tone and avoid unnecessary elaboration."
        )

        return prompt

    @staticmethod
    async def stream_llama_response(prompt: str, model: str = "llama3.2:1b", suffix: str = "") -> str:
        """
        Stream LLM response for a given prompt.

        Args:
            prompt: The prompt to send to the LLM.
            model: The model to use for generating the response.
            suffix: Optional suffix to append to the prompt.

        Returns:
            A string containing the streamed response from the LLM.
        """
        try:
            print(prompt)
            # Assuming AsyncClient.chat is asynchronous
            async for part in await AsyncClient().chat(
                    model=model,
                    messages=[{'role': 'user', 'content': prompt}],
                    stream=True
            ):
                print(json.dumps(part['message']['content']))
                yield f"data: {json.dumps(part['message']['content'])}\n\n"
        except Exception as e:
            yield f"data: Error occurred while streaming: {e}\n\n"
