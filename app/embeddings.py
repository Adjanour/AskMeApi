import asyncio
import json
from ollama import AsyncClient
from pinecone import Pinecone
from typing import List, Dict, Any
import numpy as np
from sentence_transformers import SentenceTransformer
import spacy
from typing import List, Dict
from app.utils import sanitize_id




class EmbeddingsHandler:
    def __init__(self,
                 spacy_model: spacy.language.Language,
                 sentence_transformer: SentenceTransformer,
                 pinecone_index: Pinecone.Index,
                 embedding_dim: int = 384):
        """
                 Initialize an EmbeddingsHandler with models and Pinecone index for text processing and embedding.
                 
                 Parameters:
                     spacy_model (spacy.language.Language): A spaCy language model used for text preprocessing tasks like lemmatization and stopword removal.
                     sentence_transformer (SentenceTransformer): A sentence transformer model for generating high-quality text embeddings.
                     pinecone_index (Pinecone.Index): A Pinecone vector database index for storing and retrieving vector embeddings.
                     embedding_dim (int, optional): The dimensionality of embeddings generated by the sentence transformer. Defaults to 384.
                 
                 Attributes:
                     embedding_dim (int): Stores the dimensionality of embeddings.
                     spacy_model (spacy.language.Language): Preprocesses and analyzes text.
                     sentence_transformer (SentenceTransformer): Generates text embeddings.
                     pinecone_index (Pinecone.Index): Manages vector storage and similarity search.
                 """
        self.embedding_dim = embedding_dim
        self.spacy_model = spacy_model
        self.sentence_transformer = sentence_transformer
        self.pinecone_index = pinecone_index

    def preprocess_text(self, text: str) -> str:
        """
        Preprocess the input text by converting to lowercase, lemmatizing words, and removing stopwords.
        
        Parameters:
            text (str): The input text string to be preprocessed.
        
        Returns:
            str: A cleaned text string with only lemmatized, non-stop alphabetic tokens.
        
        Notes:
            - Converts text to lowercase
            - Removes stopwords using spaCy's built-in stopword list
            - Keeps only alphabetic tokens
            - Lemmatizes remaining tokens to their base form
        """
        doc = self.spacy_model(text.lower())
        tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]
        return " ".join(tokens)

    def embed_text(self, text: str) -> np.ndarray:
        """
        Generate an embedding for the input text using the SentenceTransformer model.
        
        Parameters:
            text (str): The text to be converted into a dense vector representation.
        
        Returns:
            np.ndarray: A numpy array containing the embedding vector for the input text.
        
        Notes:
            - Uses the pre-configured SentenceTransformer model to generate embeddings
            - Converts single text input into a list to match model's input requirements
            - Returns the first (and only) embedding from the generated results
        """
        return self.sentence_transformer.encode([text])[0]

    async def store_faq_embeddings(self, faqs: List[Dict[str, str]], tenant_id: int):
        """
        Asynchronously embed and store FAQ embeddings in a Pinecone index for a specific tenant.
        
        This method preprocesses FAQ questions, generates embeddings, and stores them in Pinecone with associated metadata. Each FAQ is processed to create a unique vector ID and is indexed under the specified tenant namespace.
        
        Parameters:
            faqs (List[Dict[str, str]]): A list of FAQ dictionaries, each containing 'question' and 'answer' keys.
            tenant_id (int): The unique identifier for the tenant associated with these FAQs.
        
        Notes:
            - Preprocesses each question using lemmatization and stopword removal
            - Generates embeddings for processed questions
            - Sanitizes question text to create unique vector IDs
            - Stores embeddings in Pinecone with metadata including original question, answer, and tenant ID
        """
        processed_faqs = [
            {
                "question": faq["question"],
                "answer": faq["answer"],
                "processed_question": self.preprocess_text(faq["question"]),
            }
            for faq in faqs
        ]

        # Prepare Pinecone data and store it
        faq_vectors = []
        for faq in processed_faqs:
            question_embedding = self.embed_text(faq["processed_question"])

            # Create the Pinecone vectors to insert into the index
            vector_id = sanitize_id(faq["question"])  # Sanitize vector ID
            faq_vectors.append({
                "id": vector_id,
                "metadata": {"question": faq["question"], "answer": faq["answer"], "tenant_id": tenant_id},
                "values": question_embedding.tolist()
            })

        # Insert into Pinecone
        self.pinecone_index.upsert(vectors=faq_vectors,namespace=tenant_id)

    async def find_similar_faqs(self, query: str, tenant_id: str, top_k: int = 3) -> List[Dict[str, str]]:
        """
        Find similar FAQs for a given query using semantic similarity search.
        
        Asynchronously retrieves the most semantically similar FAQs from a Pinecone vector index for a specific tenant.
        
        Parameters:
            query (str): The user's input query to find similar FAQs for.
            tenant_id (str): Unique identifier for the tenant's namespace in the vector index.
            top_k (int, optional): Maximum number of similar FAQs to retrieve. Defaults to 3.
        
        Returns:
            List[Dict[str, str]]: A list of similar FAQs, each containing:
                - 'question': The matched FAQ question
                - 'answer': The corresponding FAQ answer
                - 'score': Semantic similarity score between the query and FAQ
        
        Notes:
            - Uses text preprocessing and embedding generation before similarity search
            - Performs vector similarity search in Pinecone index
            - Scoped to a specific tenant's namespace
        """
        # Preprocess and embed the user query
        processed_query = self.preprocess_text(query)
        query_embedding = self.embed_text(processed_query).tolist()

        # Query Pinecone using the tenant_id as the namespace
        query_response = self.pinecone_index.query(
            namespace=tenant_id,  # Use tenant_id as the namespace
            vector=query_embedding,  # Pass the query embedding
            top_k=top_k,  # Number of results to retrieve
            include_metadata=True  # Include metadata in the response
        )

        # Extract relevant results
        results = [
            {
                "question": item["metadata"]["question"],
                "answer": item["metadata"]["answer"],
                "score": item["score"]  # Similarity score
            }
            for item in query_response["matches"]
        ]

        return results


class LLMHandler:
    @staticmethod
    def generate_llama_prompt(
            similar_faqs: List[Dict[str, Any]],
            user_query: str,
            conversation_history: List[Dict[str, str]] = []
    ) -> str:
        """
            Generate a focused prompt for the LLM using FAQs and conversation history for accurate, relevant responses.
            
            Args:
                similar_faqs (List[Dict[str, Any]]): A list of similar FAQs retrieved based on semantic similarity to the user's query. 
                    Each FAQ dictionary should contain 'question' and 'answer' keys.
                user_query (str): The current user's question or input that requires a response.
                conversation_history (List[Dict[str, str]], optional): A sequential list of previous conversation messages 
                    with 'role' and 'content' keys. Defaults to an empty list.
            
            Returns:
                str: A carefully structured prompt string optimized for generating contextually relevant and precise responses 
                     from the language model, incorporating FAQ context, conversation history, and the current user query.
            
            Notes:
                - Designed to provide context-aware responses for customer support scenarios
                - Handles cases where exact FAQ matches may not exist
                - Encourages concise and direct communication
            """
        # Concatenate Q&A pairs from similar FAQs to form the context
        faq_context = "\n\n".join([f"Q: {faq['question']}\nA: {faq['answer']}" for faq in similar_faqs])

        # Format the conversation history context
        conversation_context = "\n".join(
            [f"{message['role'].capitalize()}: {message['content']}" for message in conversation_history])

        # Structured and targeted prompt
        prompt = (
            f"You are a customer support chatbot, designed to give precise, relevant answers based on the provided FAQ. "
            f"Use the FAQ entries and conversation history below to craft a direct, helpful response to the user's question. "
            f"If the exact answer isn't in the FAQ, infer the best possible answer or advise on next steps.\n\n"

            f"FAQs:\n{faq_context}\n\n"
            f"Conversation so far:\n{conversation_context}\n\n"

            f"User question: {user_query}\n\n"

            f"Please provide a clear answer based on the information above. "
            f"Use a polite, concise tone and avoid unnecessary elaboration."
        )

        return prompt

    @staticmethod
    async def stream_llama_response(prompt: str, model: str = "llama3.2:1b", suffix: str = "") -> str:
        """
        Stream LLM response for a given prompt.

        Args:
            prompt: The prompt to send to the LLM.
            model: The model to use for generating the response.
            suffix: Optional suffix to append to the prompt.

        Returns:
            A string containing the streamed response from the LLM.
        """
        try:
            print(prompt)
            # Assuming AsyncClient.chat is asynchronous
            async for part in await AsyncClient().chat(
                    model=model,
                    messages=[{'role': 'user', 'content': prompt}],
                    stream=True
            ):
                print(json.dumps(part['message']['content']))
                yield f"data: {json.dumps(part['message']['content'])}\n\n"
        except Exception as e:
            yield f"data: Error occurred while streaming: {e}\n\n"
