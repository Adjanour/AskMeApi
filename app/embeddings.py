import asyncio
import json
from ollama import AsyncClient
from pinecone import Pinecone
from typing import List, Dict, Any
import numpy as np
from sentence_transformers import SentenceTransformer
import spacy
from typing import List, Dict
from app.utils import sanitize_id




class EmbeddingsHandler:
    def __init__(self,
                 spacy_model: spacy.language.Language,
                 sentence_transformer: SentenceTransformer,
                 pinecone_index: Pinecone.Index,
                 embedding_dim: int = 384):
        """
        Initializes the EmbeddingsHandler with the required models and Pinecone index.

        Args:
            spacy_model: The spaCy model for preprocessing text.
            sentence_transformer: The SentenceTransformer model for embeddings.
            pinecone_index: The Pinecone index for storing/retrieving embeddings.
            embedding_dim: The dimension of the embeddings generated by the SentenceTransformer.
        """
        self.embedding_dim = embedding_dim
        self.spacy_model = spacy_model
        self.sentence_transformer = sentence_transformer
        self.pinecone_index = pinecone_index

    def preprocess_text(self, text: str) -> str:
        """
        Preprocess the input text: lemmatize and remove stopwords.

        Args:
            text: The input text string to preprocess.

        Returns:
            A preprocessed text string with stopwords removed and words lemmatized.
        """
        doc = self.spacy_model(text.lower())
        tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]
        return " ".join(tokens)

    def embed_text(self, text: str) -> np.ndarray:
        """
        Generate an embedding for the input text.

        Args:
            text: The input text to embed.

        Returns:
            A numpy array representing the embedding of the input text.
        """
        return self.sentence_transformer.encode([text])[0]

    async def store_faq_embeddings(self, faqs: List[Dict[str, str]], tenant_id: int):
        """
        Embed and store FAQs in Pinecone index with tenant association.

        Args:
            faqs: A list of FAQ dictionaries containing question and answer.
            tenant_id: The ID of the tenant associated with these FAQs.
        """
        processed_faqs = [
            {
                "question": faq["question"],
                "answer": faq["answer"],
                "processed_question": self.preprocess_text(faq["question"]),
            }
            for faq in faqs
        ]

        # Prepare Pinecone data and store it
        faq_vectors = []
        for faq in processed_faqs:
            question_embedding = self.embed_text(faq["processed_question"])

            # Create the Pinecone vectors to insert into the index
            vector_id = sanitize_id(faq["question"])  # Sanitize vector ID
            faq_vectors.append({
                "id": vector_id,
                "metadata": {"question": faq["question"], "answer": faq["answer"], "tenant_id": tenant_id},
                "values": question_embedding.tolist()
            })

        # Insert into Pinecone
        self.pinecone_index.upsert(vectors=faq_vectors,namespace=tenant_id)

    async def find_similar_faqs(self, query: str, tenant_id: str, top_k: int = 3) -> List[Dict[str, str]]:
        """
        Find top similar FAQs for the given query using Pinecone.

        Args:
            query: The user query to compare against stored FAQs.
            tenant_id: The tenant ID used as the namespace.
            top_k: The number of top similar FAQs to retrieve.

        Returns:
            A list of dictionaries containing the most similar FAQs and their scores.
        """
        # Preprocess and embed the user query
        processed_query = self.preprocess_text(query)
        query_embedding = self.embed_text(processed_query).tolist()

        # Query Pinecone using the tenant_id as the namespace
        query_response = self.pinecone_index.query(
            namespace=tenant_id,  # Use tenant_id as the namespace
            vector=query_embedding,  # Pass the query embedding
            top_k=top_k,  # Number of results to retrieve
            include_metadata=True  # Include metadata in the response
        )

        # Extract relevant results
        results = [
            {
                "question": item["metadata"]["question"],
                "answer": item["metadata"]["answer"],
                "score": item["score"]  # Similarity score
            }
            for item in query_response["matches"]
        ]

        return results


class LLMHandler:
    @staticmethod
    def generate_llama_prompt(
            similar_faqs: List[Dict[str, Any]],
            user_query: str,
            conversation_history: List[Dict[str, str]] = []
    ) -> str:
        """
        Generate a focused prompt for the LLM using FAQs and conversation history for accurate, relevant responses.

        Args:
            similar_faqs: A list of similar FAQs based on the user's query.
            user_query: The user's current question.
            conversation_history: A list of previous conversation messages.

        Returns:
            A formatted prompt string to send to the LLM for generating a response.
        """
        # Concatenate Q&A pairs from similar FAQs to form the context
        faq_context = "\n\n".join([f"Q: {faq['question']}\nA: {faq['answer']}" for faq in similar_faqs])

        # Format the conversation history context
        conversation_context = "\n".join(
            [f"{message['role'].capitalize()}: {message['content']}" for message in conversation_history])

        # Structured and targeted prompt
        prompt = (
            f"You are a customer support chatbot, designed to give precise, relevant answers based on the provided FAQ. "
            f"Use the FAQ entries and conversation history below to craft a direct, helpful response to the user's question. "
            f"If the exact answer isn't in the FAQ, infer the best possible answer or advise on next steps.\n\n"

            f"FAQs:\n{faq_context}\n\n"
            f"Conversation so far:\n{conversation_context}\n\n"

            f"User question: {user_query}\n\n"

            f"Please provide a clear answer based on the information above. "
            f"Use a polite, concise tone and avoid unnecessary elaboration."
        )

        return prompt

    @staticmethod
    async def stream_llama_response(prompt: str, model: str = "llama3.2:1b", suffix: str = "") -> str:
        """
        Stream LLM response for a given prompt.

        Args:
            prompt: The prompt to send to the LLM.
            model: The model to use for generating the response.
            suffix: Optional suffix to append to the prompt.

        Returns:
            A string containing the streamed response from the LLM.
        """
        try:
            print(prompt)
            # Assuming AsyncClient.chat is asynchronous
            async for part in await AsyncClient().chat(
                    model=model,
                    messages=[{'role': 'user', 'content': prompt}],
                    stream=True
            ):
                print(json.dumps(part['message']['content']))
                yield f"data: {json.dumps(part['message']['content'])}\n\n"
        except Exception as e:
            yield f"data: Error occurred while streaming: {e}\n\n"
